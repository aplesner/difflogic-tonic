base:
  seed: 42
  job_id: "baseline_mlp_run"
  wandb:
    online: true
    project: "difflogic-tonic"
    entity: null
    run_name: null
    tags: ["cifar10dvs", "mlp"]
    notes: null

data:
  name: CIFAR10DVS  # Options: NMNIST, CIFAR10DVS

  # Optional: specify cached dataset variant
  cache_identifier: null

  # Input transforms
  downsample_pool_size: null  # Max pooling downsampling (null = disabled)

  # Data augmentation (applied only to training set)
  augmentation:
    horizontal_flip_probability: 0.0  # Probability [0.0-1.0] for horizontal flip (0 = disabled)
    vertical_flip_probability: 0.0  # Probability [0.0-1.0] for vertical flip (0 = disabled)
    random_crop_padding: 0  # Padding for random crop (0 = disabled)
    salt_pepper_noise: 0.0  # Probability [0.0-1.0] to flip random pixels

train:
  epochs: 10
  learning_rate: 1e-3
  log_interval: 250
  debugging_steps: 1000
  save_model: false
  checkpoint_interval_minutes: 10.0
  model_path: models/
  device: cuda
  dtype: bfloat16 # options: float16, bfloat16, float32
  dataloader:
    batch_size: 64
    num_workers: 4
    prefetch_factor: 2
    pin_memory: true
    shuffle_train: true
  scheduler:
    enabled: false
    type: "step"
    step_size: 10
    gamma: 0.1
    min_lr: 1e-6
    factor: 0.5
    patience: 5
  early_stopping:
    enabled: false
    monitor: "val/acc"
    patience: 10
    mode: "max"
    min_delta: 0.0
  lightning:
    enable_progress_bar: true
    log_every_n_steps: 50
    gradient_clip_val: null
    accumulate_grad_batches: 1
    check_val_every_n_epoch: 1
    num_sanity_val_steps: 2

model:
  model_type: MLP
  mlp:
    hidden_size: 512
    num_hidden_layers: 2
