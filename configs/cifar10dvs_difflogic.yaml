base:
  seed: 42
  job_id: "baseline_difflogic_run"
  wandb:
    online: true  # Set to true to enable WandB logging
    project: "difflogic-tonic"
    entity: null
    run_name: null
    tags: ["cifar10dvs", "difflogic"]
    notes: null

data:
  name: CIFAR10DVS  # Options: NMNIST, CIFAR10DVS

  # Specify which cached dataset variant to use (matches prepare_data output)
  # Examples: "events_20000_overlap0_denoise5000", "time_10000_overlap0_denoise5000"
  # Leave as null to use default cache (backward compatibility)
  cache_identifier: null  # or "events_20000_overlap0_denoise5000"

  # Input transforms
  downsample_pool_size: 2  # Max pooling: 128x128 -> 64x64 (2), 128x128 -> 32x32 (4), null for no downsampling

  # Data augmentation (applied only to training set)
  augmentation:
    horizontal_flip_probability: 0.0  # Probability [0.0-1.0] for horizontal flip (0 = disabled)
    vertical_flip_probability: 0.0  # Probability [0.0-1.0] for vertical flip (0 = disabled)
    random_crop_padding: 0  # Padding for random crop (0 = disabled)
    salt_pepper_noise: 0.20  # Probability [0.0-1.0] to flip random pixels

train:
  epochs: 10
  learning_rate: 1e-2
  log_interval: 250
  debugging_steps: 1000
  save_model: false  # Enable to save model checkpoints
  checkpoint_interval_minutes: 10.0  # Deprecated with Lightning (uses callbacks instead)
  model_path: models/
  device: cuda
  dtype: float16 # options: float16, bfloat16, float32

  dataloader:
    batch_size: 64
    num_workers: 4
    prefetch_factor: 2
    pin_memory: true
    shuffle_train: true

  # Learning rate scheduler (optional)
  scheduler:
    enabled: false  # Set to true to enable scheduler
    type: "step"  # Options: step, cosine, reduce_on_plateau
    step_size: 10
    gamma: 0.1
    min_lr: 1e-6
    factor: 0.5  # for reduce_on_plateau
    patience: 5  # for reduce_on_plateau

  # Early stopping (optional)
  early_stopping:
    enabled: false  # Set to true to enable early stopping
    monitor: "val/acc"
    patience: 10
    mode: "max"  # max for accuracy, min for loss
    min_delta: 0.0

  # PyTorch Lightning trainer settings
  lightning:
    enable_progress_bar: true
    log_every_n_steps: 50
    gradient_clip_val: null  # Set to a value (e.g., 1.0) to enable gradient clipping
    accumulate_grad_batches: 1
    check_val_every_n_epoch: 1
    num_sanity_val_steps: 2

model:
  model_type: DiffLogic
  difflogic:
    architecture: "fully_connected"  # Options: fully_connected and convolutional (not implemented yet)
    num_neurons: 32000
    num_layers: 4
    connections: "random"
    grad_factor: 1.0
    tau: 10.0