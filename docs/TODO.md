# DiffLogic Tonic - Task & TODO List

This file tracks tasks, improvements, and known issues for the DiffLogic Tonic project.

## Legend
- ğŸ”´ High Priority
- ğŸŸ¡ Medium Priority
- ğŸŸ¢ Low Priority
- âœ… Completed
- ğŸš§ In Progress
- ğŸ“ Planned

---

## Phase 1: Infrastructure & Config ğŸ”´

### Hydra Configuration Migration
- [âœ…] ğŸ”´ Replace OmegaConf direct usage with Hydra
- [âœ…] ğŸ”´ Restructure configs/ with composition (base + overrides)
- [âœ…] ğŸ”´ Update main.py and prepare_data.py entry points for Hydra
- [âœ…] ğŸ”´ Update train.sh and prepare_data.sh for Hydra CLI
- [ ] ğŸŸ¡ Update SLURM job scripts to use Hydra syntax
- [ ] ğŸŸ¡ Prepare WandB sweeps integration with Hydra
- [ ] ğŸŸ¡ Update container with hydra-core dependency

**Why**: Current configs have redundancy and repetition. Hydra provides cleaner CLI overrides and better composition.

**Status**: âœ… Core migration complete! Legacy scripts in `legacy/` folder. See [docs/HYDRA_MIGRATION.md](HYDRA_MIGRATION.md).

### SLURM & Cluster Tools
- [âœ…] ğŸŸ¡ Add container rsync to train.sh (pre-flight sync)
- [âœ…] ğŸŸ¡ Create slurm_jupyter.sh for Jupyter server jobs

---

## Phase 2: Data Pipeline ğŸ”´

### Parquet Dataloader for Raw Events
- [ ] ğŸ”´ Create separate module/codebase for raw event data â†’ Parquet conversion
- [ ] ğŸ”´ Define Parquet schema: (timestamp, x, y, polarity, label, sample_id)
- [ ] ğŸ”´ Implement PyArrow-based DataLoader
- [ ] ğŸ”´ Integrate with current caching system
- [ ] ğŸŸ¡ HuggingFace datasets integration (replace broken Tonic downloads)

**Why**: Tonic datasets have broken download links. Parquet provides efficient columnar storage for event data and better interoperability.

### Cython Optimization
- [ ] ğŸŸ¡ Cythonize Tonic ToFrame function
- [ ] ğŸŸ¡ Cythonize key augmentation operations
- [ ] ğŸŸ¡ Add Python fallback if Cython unavailable
- [ ] ğŸŸ¢ Benchmark performance improvements
- [ ] ğŸŸ¢ Package as separate importable module

**Why**: Event processing (ToFrame, augmentations) is computationally expensive. Cython can provide significant speedups.

---

## Phase 3: Classical CV Experiments ğŸŸ¡

### RGB Dataset Experiments
- [ ] ğŸŸ¡ Extend classic_cv.ipynb for CIFAR10/100 experiments
- [ ] ğŸŸ¡ Add configs for ImageNette dataset
- [ ] ğŸŸ¡ Implement classical CV filter baselines (Gabor, HOG, SIFT)
- [ ] ğŸŸ¢ Compare with event-based approaches

**Why**: Classical CV filters on RGB provide baselines for understanding event-based performance.

---

## Phase 4: Temporal Processing ğŸŸ¡

### Short Frame Classification & Ensemble
- [ ] ğŸŸ¡ Implement dataset format: [n_samples, n_frames, C, H, W]
- [ ] ğŸŸ¡ Support multiple short frames per sample (1k-5k events each vs current 15k)
- [ ] ğŸŸ¡ Implement ensemble/voting aggregation strategy
- [ ] ğŸŸ¡ Add configs for frame count and aggregation method
- [ ] ğŸŸ¢ Analyze impact of event aggregation window sizes

**Goal**: Aggregate fewer events per frame (1k-5k instead of 15k), predict per-frame, then ensemble using voting or other strategies.

**Data structure**: Split 15k events into multiple short frames â†’ predict on each â†’ aggregate predictions.

### Recurrent Encoder Network
- [ ] ğŸŸ¡ Implement GRU-based encoder model
- [ ] ğŸŸ¡ Add simple decoder for logits from representation
- [ ] ğŸŸ¡ Process temporal sequence of short frames
- [ ] ğŸŸ¡ Integrate as new model type (alongside DiffLogic, CNN, MLP)
- [ ] ğŸŸ¢ Add time-as-channel variant (flatten with time channel)

**Goal**: Introduce time relationships between samples using recurrent models for per-sample predictions.

---

## Phase 5: Advanced Features ğŸŸ¢

### Learned Connections for DiffLogic
- [ ] ğŸŸ¢ Literature review: LUT learning for FPGAs
- [ ] ğŸŸ¢ Make DiffLogic connections learnable (vs random/fixed)
- [ ] ğŸŸ¢ Implement learnable connection parameters
- [ ] ğŸŸ¢ Maintain backward compatibility with existing models

**Why**: Papers show learned LUTs improve FPGA performance. Apply similar concepts to DiffLogic.

### CUDA Integration (Simon's LUT6 Functions)
- [ ] ğŸŸ¢ Integrate Simon's CUDA functions for learned connections
- [ ] ğŸŸ¢ Implement fast LUT6 model inference
- [ ] ğŸŸ¢ Benchmark CUDA vs PyTorch implementations
- [ ] ğŸŸ¢ Add conditional compilation/import

**Why**: Simon implemented CUDA functions for learned connections and LUT6 models. Leverage for performance.

---

## Completed Features âœ…

### Training Pipeline
- [âœ…] Add support for learning rate schedulers (PyTorch Lightning)
- [âœ…] Implement early stopping mechanism (PyTorch Lightning)
- [âœ…] Add gradient clipping options (PyTorch Lightning)
- [âœ…] Support for mixed precision training optimization
- [âœ…] Add validation metrics tracking (F1, precision, recall)
- [âœ…] Migrate to PyTorch Lightning for cleaner training code
- [âœ…] Add wandb integration (PyTorch Lightning WandbLogger)

### Data Processing
- [âœ…] Do test/train split before processing samples
- [âœ…] Include metadata extraction pipeline for frame duration analysis
- [âœ…] Refactor data preparation for code reuse
- [âœ…] Use torchvision transforms v2 for data augmentation
- [âœ…] Add configurable data augmentation (flip probability, random crop, etc.)

### Models
- [âœ…] Discretize difflogic models

### Checkpointing
- [âœ…] Implement automatic checkpoint cleanup (Lightning ModelCheckpoint)
- [âœ…] Add support for loading specific checkpoint by timestamp

---

## Backlog & Future Work

### Models
- [ ] ğŸŸ¢ Implement IWP variant
- [ ] ğŸŸ¢ Implement CLGNs

### Data Processing
- [ ] ğŸŸ¢ Add support for additional neuromorphic datasets (DVS-Gesture, N-Caltech101)
- [ ] ğŸŸ¢ Add dataset statistics and visualization tools

### Infrastructure
- [ ] ğŸŸ¢ Export final models to ONNX format
- [ ] ğŸŸ¢ Validate checkpoint recovery with different configurations
- [ ] ğŸŸ¢ Review error handling for missing environment variables
- [ ] ğŸŸ¢ Add performance benchmarking tests
- [ ] ğŸŸ¢ Add docstrings to all public functions and classes

---

## Priority Guidelines
- ğŸ”´ **High Priority**: Critical for core functionality or blocking other work
- ğŸŸ¡ **Medium Priority**: Important improvements or useful features
- ğŸŸ¢ **Low Priority**: Nice-to-have enhancements or quality-of-life improvements
